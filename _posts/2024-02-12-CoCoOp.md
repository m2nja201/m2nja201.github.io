---
title:  "[Paper Review] CoCoOp :: Conditional Prompt Learning for Vision-Language Models"
excerpt: "Conditional Prompt Learning for Vision-Language Models (CoCoOp) 논문 리뷰"

toc: true
toc_sticky: true

categories:
  - ComputerVision 
  - PaperReview
  - MultiModal

---

논문 <font style="color:hsl(27, 100%, 43%)">Conditional Prompt Learning for Vision-Language Models</font>을 읽고, 
최대한 다른 분들이 보셨을 때 이해가 잘 되도록 요약 및 핵심 내용 설명 위주로 Paper Review를 진행할 예정입니다.

보시기 전에 ["CoOp 논문 리뷰"](https://m2nja201.github.io/computervision/paperreview/multimodal/CoOp/)를 먼저 보시는 것을 추천드립니다.
자유롭게 피드백 남겨주세요 :)

## 🔍 Abstract 및 Introduction
### 기존 연구의 문제점
[CoOp](https://m2nja201.github.io/computervision/paperreview/multimodal/CoOp/)에서도 언급을 하였지만, 기존의 학습에서는 label이 <font style="color:hsl(27, 100%, 43%)">이산화</font>되고, 각 카테고리를 무작위로 초기화된 weight vector와 연결하여, 같은 카테고리를 포함하는 이미지와의 거리를 최소화하도록 학습된다.

이 방법은 사전에 정의된 카테고리 목록에 학습을 <font style="color:hsl(27, 100%, 43%)">한정시키</font>며, 보지 못했던 <font style="color:hsl(27, 100%, 43%)">새로운 카테고리</font>에 대한 확장이 불가능하게 만드는 폐쇄적인 학습 방법이라고 할 수 있다.

<hr>

### Vision Language Model
CLIP, ALIGN과 같은 VL-model에서는 분류 가중치가 prompting을 통해 text encoder로부터 생성된다.

즉, 프롬프트 템플릿을 텍스트 인코더의 입력으로 사용할 수 있으며, 실제 클래스 이름으로 “``CLASS``” 토큰을 채워 분류를 위한 클래스별 가중치를 합성할 수 있다. Discrete label과 비교했을 때, 보다 더 개방적인 시각 개념을 탐색할 수 있게 되었다.

이러한 모델들을 <font style="color:hsl(27, 100%, 43%)">downstream task</font>에 효율적으로 적응시키기 위한 연구들이 시작되었지만, 대규모 VL model들은 고용량을 목적으로 설계되었으며, 수십억 개의 매개변수를 가지기 때문에 일반 fine-tuning은 실용적인 방안이 될 수 없다는 것을 알게 되었다.

<hr>

### 기존 Prompting의 문제점
직접 prompt를 설계하는 <font style="color:hsl(27, 100%, 43%)">prompt engineering</font>은 optimal prompt를 보장하지 않고, 많은 경우의 수를 통해 찾아야 한다.
즉, 매우 시간 소모적이며 비효율적이다.

<hr>

### CoOp의 등장
NLP의 <font style="color:hsl(27, 100%, 43%)">prompt learning</font> 개념을 VL model에 적용하는 것을 탐구하였다.
[Context Optimization (CoOp)](https://m2nja201.github.io/computervision/paperreview/multimodal/CoOp/)의 아이디어는 간단히 다음과 같다.
- prompt 내의 context word를 <font style="color:hsl(27, 100%, 43%)">학습 가능한 벡터 세트</font>로 전환한다.
- CLIP의 base 모델은 고정시킨다.
- continous prompt를 사용한다.

고안된 CoOp은 tuned manual prompt (수동 프롬프트) 보다 더 높은 성능을 냈었다.

### CoOp의 치명적인 문제점 발견
그러나, 같은 task 내에서도 <font style="color:hsl(27, 100%, 43%)">unseen class</font>들에 대해 일반화 성능이 떨어지는 것을 확인하였다.

<center>
<img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/3b1a8545-c964-4747-9399-24e87178c497" width="70%" height="70%"></center>

위와 같이 ``Arrival gate``와 ``Cathedral``과 같이 <font style="color:hsl(27, 100%, 43%)">Dataset에 존재하는 base class</font>에 대한 성능은 CoOp의 성능이 좋은 것을 확인할 수 있다.

<center>
<img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/d27e6dd9-d61f-48d9-af02-14a259d09b63" width="70%" height="70%"></center>

그러나 같은 범주인 scene understanding에 속하여도, <font style="color:hsl(27, 100%, 43%)">Dataset에 존재하지 않는 wider unseen class</font>에서는 일반화 요소를 포착하지 못하는 것을 볼 수 있다.

이러한 문제는 <font style="color:hsl(27, 100%, 43%)">CoOp의 정적 설계</font>에 의해 발생한다고 주장한다. 즉 한 번 학습되면 context가 <font style="color:hsl(27, 100%, 43%)">고정</font>되기 때문에 <font style="color:hsl(27, 100%, 43%)">특정한 class set</font>에만 최적화 되는 것이다.

<hr>

### CoCoOp의 소개
일반화 문제를 해결하기 위해, <font style="color:hsl(27, 100%, 43%)">Conditional prompt learning</font>인 <font style="color:hsl(27, 100%, 43%)">Conditional Context Optimization (CoCoOp)</font> 방법을 소개한다.

#### ➡️ 핵심 아이디어
- prompt를 고정시키지 않고, 각 <font style="color:hsl(27, 100%, 43%)">input instance (image)</font>에 따라 prompt를 <font style="color:hsl(27, 100%, 43%)">조건화</font>한다.
- CoOp을 확장하여 각 이미지에 대해 <font style="color:hsl(27, 100%, 43%)">input-conditional token</font>을 생성하는 <font style="color:hsl(27, 100%, 43%)">경량 신경망</font>을 추가로 학습하며, 이는 학습 가능한 context vector와 결합된다.

#### ➡️ <font style="color:hsl(27, 100%, 43%)">instance-conditional prompt</font>를 통해 더 일반화 시킬 수 있는 이유
특정한 class만을 위하기 보다, 각 <font style="color:hsl(27, 100%, 43%)">instance들을 특정</font> 짓도록 최적화 되어 있기 때문이다.

<hr>

### Contributions
- 존재하지 않던 <font style="color:hsl(27, 100%, 43%)">unseen classes</font>에 대해서 manual prompts 간의 격차를 매우 줄일 수 있었고, 보다 더 좋은 성능을 얻을 수 있었다.
- CoOp보다 더 강한 <font style="color:hsl(27, 100%, 43%)">도메인 일반화</font> 성능을 도출 할 수 있다.
- 학습된 context가 극적으로 다른 클래스를 가진 다른 작업으로 전환될 때, CoOp의 성능을 명확한 margin으로 능가하기 때문에, 더 큰 규모에서 성공할 수 있는 잠재력을 가지고 있다.

<br>

## 🔍 Related Works
※ [CoOp 논문](https://m2nja201.github.io/computervision/paperreview/multimodal/CoOp/)과 거의 유사하기 때문에 해당 게시글을 참고해주세요.

<br>

## 🔍 Methodology
전체적인 architecture는 다음과 같다.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/3c887494-7d7b-4f87-ac66-dd156a1d576f" width="70%"></center>

<hr>

### CLIP (contrastive language-image pre-training) backbone 사용

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/6609e8e6-ff21-4982-9f13-82d96e227b39" width="50%"> <img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/f9d5d5e8-6831-4e96-9558-acce99cd5dcf" width="40%"></center>

- 두 개의 인코더 (Image Encoder, Text Encoder) 를 사용하여 구축했다.
- Image Encoder는 ResNet 혹은 ViT이며, 이미지를 특징 벡터로 변환한다.
- Text Encoder는 Transformer로 단어 토큰의 시퀀스를 입력 받아 벡터화된 표현을 생성한다.
- Loss : 두 modality의 결합된 embedding space를 학습하기 위해 contrastive loss를 채택하였다.
  - matched pair에 대해선, <font style="color:hsl(27, 100%, 43%)">cosine similarity</font>를 최대화
  - unmatched pair에 대해선, cosine similarity를 최소화

<hr>

### CoOp 적용
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/499f1dcf-3a29-413f-a74e-e8c5d00799dc" width="50%"></center>

- 끝까지 학습할 수 있는 <font style="color:hsl(27, 100%, 43%)">continuous vector</font>를 사용하여 각 context token을 모델링한다.

``a photo of a ~``라는 context를 사용하는 대신, word embedding과 동일한 차원을 가진 <font style="color:hsl(27, 100%, 43%)">학습 가능한 context vector`` ``{v1, v2, ... , vM}``을 사용하는 것이다.
즉, 완성된 prompt는 ``{v1, v2, ... , vM, ci}``이다.

<hr>

### CoCoOp 적용
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/bfc925cb-b1d3-4a4f-afca-748aa11ccef0" width="40%"> <img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/ba73b852-86e0-49d0-b1a0-e0199c5bd9db" width="30%"></center>

- 특정 클래스 집합에서 <font style="color:hsl(27, 100%, 43%)">instance-conditional context</font>가 focus를 이동시켜 <font style="color:hsl(27, 100%, 43%)">overfitting</font>을 줄이고, 각 input instance에 초점을 맞춤으로써 일반화의 능력이 더 높아진다.

#### ➡️ 구현 방식
M개의 신경망을 구축하여, M개의 context token을 얻는 것을 처음 고안하였으나, 이는 신경망 하나의 크기에 M배가 필요하다. 그래서 본 논문에서는 효율적인 매개변수 설계를 다음과 같이 제안한다.

M개의 <font style="color:hsl(27, 100%, 43%)">context vector</font> 위에 각 입력에 대한 <font style="color:hsl(27, 100%, 43%)">conditional token(vector)</font>를 생성하는 경량 신경망인 <font style="color:hsl(27, 100%, 43%)">Meta-Net</font>를 학습하여 context vector와 결합시킨다.

#### ➡️ Meta Net
- Input : Image Encoder에 의해 생성된 출력 feature
- 두 층의 병목 구조 (Linear-ReLU-Linear) 로 구축
- 은닉층에서는 입력 차원을 16배 줄임

<br>

## 🔍 Experiments
다음 세 가지와 부수적인 내용에 대해 실험을 진행하였다.
1. Dataset 내 base class에서 새로운 class로의 일반화
2. Dataset간의 전이 (cross-dataset)
3. 도메인 일반화 능력

### Dataset과 Training Detais
#### Dataset
ImageNet, Caltech101, OxfordPets, StanfordCars, Flowers102, Food101, FGVCAircraft, SUN397, UCF101, DTD, EuroSAT

#### Training Details
CLIP에서 가장 좋은 vision backbone인 ViT-B/16을 사용하였다. CoOp에서는 더 짧은 context length와 좋은 initialization이 더 나은 성능과 도메인 변화에 대한 강인성을 이끌어낼 수 있다고 하였다. 그래서 context length를 4로 고정하고, ``a photo of a``의 사전 훈련된 word embedding을 사용하여 초기화했다고 한다.
> 근데 실제로 CoOp 논문을 보면, 초기화 방법에서의 무작위 초기화와 단어 임베딩 기반 초기화의 성능차이는 거의 없던 것으로 확인할 수 있어서 의문이다.

<hr>

### [1] Generalization from Base to New Classes
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/a2b29212-f35f-4f3f-9e19-f5e1039fd1f4" width="30%"> <img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/655e711a-3587-40dd-92ac-d30bb0087865" width="50%"></center>

왼쪽 표를 보면 알 수 있듯이, Base class와 New class에 대한 정확도 수치가 현저하게 다르다. 11개 Dataset에 대한 평균 성능 중 New class에 대한 정확도는 CLIP이 가장 높았지만, CoOp의 일반화 성능이 나머지 두 방법에 대해 현저히 떨어지는 것을 확인할 수 있었다.

본 <font style="color:hsl(27, 100%, 43%)">Base와 New의 평균 값</font>은 FGVCAircraft를 제외하고 <font style="color:hsl(27, 100%, 43%)">CoCoOp</font>이 가장 높은 성능을 보였다. 즉, CoCoOp은 New Class에서의 정확도를 ``63.22% -> 71.69%``로 향상 시켰다.

오른쪽 그래프는 CoOp에 비해 높아진 CoCoOp의 성능 척도를 보여준다.

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/770ba1b6-f5c3-453d-8837-6ea2afa68367" width="50%"></center> 

본 표는 Base class에 대한 CoCoOp의 성능 향상 척도를 보여주는데, Base class에 대해서는 CoOp보다 성능이 낮은 것을 확인할 수 있다. 이는 CoOp이 기본 클래스에 특화되어 최적화 하는 반면, CoCoOp은 전체 작업에 걸쳐 일반화를 얻기 위해 <font style="color:hsl(27, 100%, 43%)">각 instance에 대해 최적화</font>하여 타당한 결과이다.

그러나 주목할 점은, Base class에 대해 낮아진 정확도가 6/9개의 dataset에서 3% 미만이었고, 이는 <font style="color:hsl(27, 100%, 43%)">unseen class</font>에서의 이득으로 상쇄된다.

<hr>

### [2] Cross-Dataset Transfer
데이터셋의 기본적인 사항이 객체 인식에서의 ``질감 분류``와 같이 완전히 달라질 수 있기 때문에 충분히 도전적인 문제이다.

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/e36e253a-3ef2-47e9-8bf8-7a57e1fb1dab" width="60%"></center> 

- Source인 ``ImageNet``은 상당량의 개 품종이 있다.
  - 개 품종을 포함하는 ``Caltech101``과 ``OxfordPets``에서 좋은 성능을 내는 것은 타당하다.
- ``FGVCAircraft``와 다양한 질감을 포함하는 ``DTD``처럼 <font style="color:hsl(27, 100%, 43%)">Dataset 유사도</font>가 떨어진 전문화된 카테고리를 가진 데이터셋에서도 성능은 낮아도, CoOp보다 <font style="color:hsl(27, 100%, 43%)">강력한 전이성</font>을 보인다.

<hr>

### [3] Domain Generalization
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/783cffbd-df39-4eb6-bb8a-7cc3f7f3297d" width="60%"></center>

CoOp과 CoCoOp 모두 CLIP에 비해 높은 <font style="color:hsl(27, 100%, 43%)">도메인 일반화 성능</font>을 보이며, ``ImageNetV2``를 제외한 세 가지 데이터셋에서 CoCoOp이 CoOp을 능가하는 성능을 보이는 것을 확인할 수 있다.

즉, <font style="color:hsl(27, 100%, 43%)">instance-conditional prompts</font>가 도메인 일반화 가능성이 더 높다는 것을 확인시켜준 것이다.

<hr>

### [4] Further Analysis
#### ➡️ Initialization
이는 <font style="color:hsl(27, 100%, 43%)">word embedding 기반</font> 초기화와 <font style="color:hsl(27, 100%, 43%)">무작위</font> 초기화를 비교한 것이다. 

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/32ff58e7-b4be-4e06-a77e-f0fa8acda71b" width="40%"></center>

CoOp에서는 초기화 방법의 성능 차이가 크지 않았던 반면, CoCoOp에서는 적절한 초기화가 Base class와 New class 모두에 더 유익하다는 것을 확인할 수 있다.

#### ➡️ Context Length
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/0670ab0f-8446-4f7b-bb4f-666f089b1235" width="40%"></center>

Base class에 대해서는 context length에 대한 변화가 크지 않지만, <font style="color:hsl(27, 100%, 43%)">New class</font>에서는 <font style="color:hsl(27, 100%, 43%)">더 긴 </font> context length를 가진 모델이 더 좋은 성능을 보인다.

그래프 (a)와 (b)를 모두 보았을 때, [ length=8, random init. ] 방법이 [ length=4, word embedding init. ] 보다 더 나은 성능을 보인다. 즉, CoCoOp에서는 context length가 큰 영향을 주는 것으로 보인다.

#### ➡️ Bigger CoOp과의 비교 (매개변수)



