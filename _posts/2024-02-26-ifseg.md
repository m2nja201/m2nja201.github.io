---
title:  "[ë…¼ë¬¸ ë¦¬ë·°] IFSeg :: Image-Free Semantic Segmentation via Vision-Language Model"
excerpt: "Image-Free Semantic Segmentation via Vision-Language Model (IFSeg) ë…¼ë¬¸ ë¦¬ë·°"

toc: true
toc_sticky: true

categories:
  - ComputerVision 
  - PaperReview
  - MultiModal

use_math: true
header:
  teaser: https://github.com/m2nja201/m2nja201.github.io/assets/80443295/0bb5f9f3-9882-4f88-a279-dd54dd978836

---

ë…¼ë¬¸ <font style="color:hsl(27, 100%, 43%)">Image-Free Semantic Segmentation via Vision-Language Model</font>ì„ ì½ê³ , 
ìµœëŒ€í•œ ë‹¤ë¥¸ ë¶„ë“¤ì´ ë³´ì…¨ì„ ë•Œ ì´í•´ê°€ ì˜ ë˜ë„ë¡ ìš”ì•½ ë° í•µì‹¬ ë‚´ìš© ì„¤ëª… ìœ„ì£¼ë¡œ Paper Reviewë¥¼ ì§„í–‰í•  ì˜ˆì •ì…ë‹ˆë‹¤.

ë…¼ë¬¸ì—ì„œì˜ ìˆœì„œë¥¼ ì¼ì •í•˜ê²Œ ì§€í‚¤ëŠ” ê²ƒë³´ë‹¤, ê° ëª©ì°¨ì—ì„œ í•„ìš”í•  ê²ƒ ê°™ë‹¤ ìƒê°í•˜ëŠ” ë¶€ë¶„ì„ ë¨¼ì € ì–¸ê¸‰í•˜ê±°ë‚˜, ë’· ë¶€ë¶„ì—ì„œ ì–¸ê¸‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³´ì‹œê³  í”¼ë“œë°±ë„ ììœ ë¡­ê²Œ ì£¼ì…¨ìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤!

## ğŸ§© Abstract & Introduction
- ìƒˆë¡œìš´ ê°œë…ë“¤ì— ëŒ€í•´ VL model pre-trainingì´ ê°ê´‘ ë°›ê³  ìˆì§€ë§Œ, **segmentation**ì— ìˆì–´ì„  ë§ì´ íƒìƒ‰ë˜ì§€ ì•Šì•˜ë‹¤.
- downstream taskì— ì ì‘ì‹œí‚¬ ë•Œ ì¶”ê°€ì ì¸ **training image**ë‚˜ **segmentation annotation**ì´ í•„ìš”í•˜ë‹¤ëŠ” ë¬¸ì œì ì´ ìˆë‹¤.
    â€” ì›¹ ì´ë¯¸ì§€ì™€ ê°™ì´ task specific training imageë‚˜ labelì´ ì—†ëŠ” ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì— ë°ì´í„°ë¥¼ ì‰½ê²Œ êµ¬í•  ìˆ˜ ì—†ë‹¤.

### Their Approach

â­ **target semantic category**ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì–´ë– í•œ task specific imageì™€ annotation ì—†ì´ **semantic segmentation**ì„ ì§„í–‰í•˜ëŠ” ê²ƒ â‡’ **Image-Free** Semantic Segmentation [**IFSeg**]

ì•„ë˜ ì‚¬ì§„ì€ categoryì˜ ì¢…ë¥˜ë§Œ promptë¡œ ì£¼ì–´ì§€ê³ , ì›¹ì—ì„œ ë‹¤ìš´ ë°›ì€ ì´ë¯¸ì§€ì´ê¸° ë•Œë¬¸ì— ì•„ë¬´ëŸ° ì •ë³´ê°€ ì—†ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ë‹¤ìŒê³¼ ê°™ì´ Segmentation ì„±ëŠ¥ì´ ë†’ë‹¤.

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/8376b58e-a378-4bed-b425-1defa638fc59" width="50%"></center>


**ë°©ë²•**ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. **word token**ì„ ì‚¬ìš©í•˜ì—¬ VL ê¸°ë°˜ì˜ image-segmentation ìŒì„ **ì¸ê³µì **ìœ¼ë¡œ ìƒì„±
        
    > ğŸ” **ì¸ê³µì ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆëŠ” ì´ìœ ** : pre-trained VL modelì€ visual tokenê³¼ text tokenì´ ê³µí†µ spaceì—ì„œ **ê°€ê¹ê²Œ** ìœ„ì¹˜í•˜ê¸° ë•Œë¬¸ì—, artificial word mapì´ ì‹¤ì œ ì´ë¯¸ì§€ inputì„ ëŒ€ì²´í•  ìˆ˜ ìˆë‹¤.
    
    ì¦‰, **semantic categoryì˜ ë‹¨ì–´ ì„¸íŠ¸**ê°€ embedding spaceì—ì„œ VL ëª¨ë¸ì— ëŒ€í•œ **artificial image**ë¡œ ê¸°ëŠ¥í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ
        
2. segmentation taskë¡œ VL model ì—…ë°ì´íŠ¸

### Contributions

- target semantic categoryë§Œ ì£¼ì–´ì§€ê³ , ì–´ë– í•œ task specific imageì™€ annotation ì—†ì„ ë•Œ, ë³¸ categoryë¥¼ ì„¸ë¶„í™”í•˜ëŠ” **IFSeg**ë¥¼ ì†Œê°œí•˜ì˜€ë‹¤.
- ì˜¤ì§ semantic segmentation categoryë§Œ ì£¼ì–´ì§„ **weaker supervision**ì„ ì‚¬ìš©í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , ê¸°ì¡´ baselineë“¤ì„ ëŠ¥ê°€í•  ìˆ˜ ìˆë‹¤.
- COCO Stuff, ADE20K ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ê³¼ í›ˆë ¨ ìš©ëŸ‰ì„ ë¹„êµí•˜ì—¬ íš¨ìœ¨ì ì¸ ì„±ëŠ¥ì„ ë„ì¶œí•¨ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.

<br>

## ğŸ§© Method
**Base** : **self-supervised** ë°©ì‹, VL encoder-decoder model + IFSeg ëª¨ë¸, semantic segmentation ì§„í–‰

### 1. VL Encoder-Decoder Architecture
#### â¡ï¸ 1.1. Data Format
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/68f2be96-d7b6-40fa-9faf-9cc51379d904" width="60%"></center>
sequence dataë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•œë‹¤. (**x** : sequence data, **$$L_x$$** : length of sequence data, **$$e_x$$** : Dì°¨ì› vector ê³µê°„ì—ì„œì˜ embedding)

1. raw image-text($$Ï‡_I, Ï‡_T$$)ë¥¼ í† í°ì˜ **sequence**ë¡œ tokenizingí•˜ì—¬ ì²˜ë¦¬í•œë‹¤.
  - ì´ë•Œ, $$ Ï‡_T $$ëŠ” ì‚¬ì „ì— ì •ì˜ëœ Nê°œì˜ ë‹¨ì–´ **dictionary** $$ V = {(v_0,..., v_{N-1})} $$ì—ì˜í•´ tokeninzing ë¨.
  - $$Ï‡_I$$ëŠ” image backbone($$f_{img}$$)ì´ ë„ì…ë˜ì–´ ë³€í˜•ëœ í›„, ì¶œë ¥ ì‚¬ì´ì¦ˆì— ë§ê²Œ linear layerê°€ ì ìš©ëœë‹¤.
  <center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/ce817efe-7ba3-4bd9-847c-ac74115f76a9" width="50%"></center>
2. $$Ï‡_I, Ï‡_T$$ë¥¼ ê²°í•©í•œë‹¤.

<br>

#### â¡ï¸ 1.2. VL Model architecture
multi modal source **`x`**ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í•™ìŠµëœ ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ target **`y`**ë¥¼ ì˜ˆì¸¡í•œë‹¤.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/87ce94cb-a5ae-4dff-84fd-00f722f50e77" width="60%"></center>

- **Encoder : $$f_{enc}$$**ëŠ” **self-attention** mechanismì„ í†µí•´ $$e_x$$ë¥¼ $$x$$ì˜ embeddingìœ¼ë¡œ ì œê³µí•œë‹¤. ì¦‰, **self-attention**ì„ í†µí•´ ê° ë‹¨ì–´ê°€ ë¬¸ì¥ì˜ ë‹¤ë¥¸ ë¶€ë¶„ê³¼ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.
- **Decoder : $$f_{dec}$$**ëŠ” **self-attention**ê³¼ **cross-attention**ë¥¼ í†µí•´ encoderì˜ ì¶œë ¥ê³¼ decoderì—ì„œì˜ ìƒíƒœë¥¼ ì£¼ëª©í•œë‹¤.

<br>

### 2. Semantic Segmentation via Encoder-Decoder
> ğŸ” **Semantic Segmentation** : ê´€ì‹¬ ìˆëŠ” Mê°œì˜ **Semantic category**ë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê° ì´ë¯¸ì§€ì˜ **ì˜ì—­**ì— ëŒ€í•œ **category word**ë¥¼ decodingí•˜ëŠ” ê²ƒ

ê·¸ëŸ¬ë‚˜, íŠ¹ì • semantic category (ex : `giraffe`) ê°€ dictionary Vì—ì„œ **ë‹¤ì¤‘ì˜ subword** (ex : `_gir`, `affe`)ë¡œ í† í°í™”ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë²ˆê±°ë¡­ë‹¤.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/2b84a06f-7a8c-47fe-a789-bfce5b7c0ec4"></center>

<br>

#### â¡ï¸ 2.1. ìœ„ ë¬¸ì œì  í•´ê²° ë°©ë²•
- `giraffe`ì™€ ê°™ì€ categoryë“¤ì„ **ì„ì‹œ ì¶”ê°€ ë‹¨ì–´**ë¡œ ì²˜ë¦¬í•œ ë’¤, **sub word tokenë“¤ì˜ í‰ê·  ì„ë² ë”©**ì„ ì„ë² ë”© í–‰ë ¬ Eì— ì¶”ê°€í•œë‹¤.
- ê·¸ëŸ¬ê³  ë‚œ ë’¤, ê° ì¹´í…Œê³ ë¦¬ë“¤ì€ í•˜ë‚˜ì˜ wordë¡œ ì¸ì‹ë˜ê²Œ ëœë‹¤. ì´ë•Œ ìµœì¢… ë²”ì£¼ ì§‘í•©ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. $$ V_{seg} = $$ $$\{v^{'}_0, ... , v^{'}_{M-1}\} $$

<br>

#### â¡ï¸ 2.2. Prompt Design
text token xTëŠ” semantic segmentation ì‘ì—…ì˜ ì„¸ë¶€ì‚¬í•­ì„ ì§€ì‹œí•˜ê¸° ìœ„í•œ promptë¡œ ì œê³µë  ìˆ˜ ìˆë‹¤. ì¦‰ **ì‘ì—… ì„¤ëª…**ê³¼ **target classì˜ ëª©ë¡**ì´ë‹¤. ì´ë•Œ VQA ì‘ì—…ì—ì„œ â€˜task description + category enumerationâ€™ protocolì„ ë”°ë¥¸ë‹¤.

> **ğŸ” VQA (Visuial Question Answering)** 
- **ì‘ì—… ì„¤ëª…** : â€˜What is the segmentation map of the image?â€ ê°™ì€ ê²ƒ . (ìˆ˜í–‰í•´ì•¼í•  ì‘ì—… ì¢…ë¥˜)
- **ì¹´í…Œê³ ë¦¬ ë‚˜ì—´** :  ëŒ€ìƒì´ ë˜ëŠ” ì¹´í…Œê³ ë¦¬ë“¤ì„ ë‚˜ì—´ . â€œobject : giraffe, grassâ€ì™€ ê°™ì´

<br>

### 3. Image-Free Semantic Segmentation
â­ **Main Idea** : VL pretraining ë™ì•ˆ ì‹¤ì œ ì´ë¯¸ì§€ tokenê³¼, ìƒì‘ë˜ëŠ” **semantic category word token**ì€ ê³µìœ ëœ ì„ë² ë”© ê³µê°„ì—ì„œ **ê·¼ì ‘í•˜ê²Œ ìœ„ì¹˜í•  ê°€ëŠ¥ì„±**ì´ ë†’ê¸° ë•Œë¬¸ì— ë‹¤ìŒ ë‚´ìš©ì´ ê°€ëŠ¥

- ì£¼ì–´ì§„ ë‹¨ì–´ í† í°ë“¤ì„ ì‚¬ìš©í•˜ì—¬ **artificial image token**ì„ ìƒì„±í•œë‹¤.
- VL ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ì—¬ í•´ë‹¹ ë‹¨ì–´ í† í°ë“¤ì„ self-supervised ë°©ì‹ìœ¼ë¡œ segmentationì„ ì§„í–‰í•œë‹¤.
> ğŸ” **Artificial Image Token** : imageì™€ segmentation labelì„ ìŒìœ¼ë¡œ í•˜ëŠ” í•™ìŠµ ë°ì´í„°

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/8daf2ab4-8f71-4330-b7ac-bb5ebe7d66d7"></center>

<br>

#### â¡ï¸ 3.1. Constructing Artificial Image Token
Mê°œì˜ category ë‹¨ì–´ ì§‘í•© **$$V_{seg}$$**ì—ì„œ **artificial training data**(image-segmentation token ìŒ)ì„ êµ¬ì„±í•˜ê³ , ë‹¨ì–´ë“¤ì€ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ì–´ **grid map**ì„ í˜•ì„±í•˜ëŠ” ë°ì— ì‚¬ìš©ëœë‹¤.

- ì´ˆê¸° Grid sizeëŠ” ì •í•´ì§„ ë²”ìœ„ ë‚´ì—ì„œ **ë¬´ì‘ìœ„**ë¡œ ê²°ì •ë˜ë©°, ì´í›„ ì´ë¯¸ì§€ ê³µê°„ í•´ìƒë„(**H*W**)ì— ë§ê²Œ ì¡°ì •ëœë‹¤.
- ì´ë•Œ, **ì´ì›ƒ ë³´ê°„ë²•(nearest neighbor)**ì´ ì‚¬ìš©ëœë‹¤. ì´ëŠ” ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„° ì—†ì´ë„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.
    
    $$
    v_{IFSeg} = [V^0_{IFSeg}, ... , v^{(H*W-1)}_{IFSeg}]
    $$
    
- ë‹¤ì–‘í•œ **random map**ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ **up-sample** í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.

> ğŸ” **Grid map** :  imageì˜ segmentationì„ í•™ìŠµí•˜ê¸° ìœ„í•œ **artificial token**ì˜ ë°°ì—´

<br>

#### â¡ï¸ 3.2. Post-processing for image-free segmentation [í›„ì²˜ë¦¬]
ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì—†ëŠ” ìƒí™©ì—ì„œ ë°œìƒí•˜ëŠ” **train dataì™€ evalutation data ê°„ì˜ ì°¨ì´**ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì œì‹œí•œë‹¤.

- image backboneì„ ê¸°ë°˜ìœ¼ë¡œ í•œ **ì¶œë ¥ í™•ë¥ ê°’ì„ í‰ê· ** ë‚´ëŠ” ê²ƒì´ segmentation í’ˆì§ˆì„ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ì˜€ë‹¤.
    - cosine ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ íŠ¹ì„±ì˜ **K-nearest neighbor** íƒìƒ‰ì„ í•œë‹¤.
    - ì´ë“¤ì˜ **í™•ë¥ ê°’ì„ í‰ê· **í•˜ì—¬, ê° í”½ì…€ì— ëŒ€í•œ segmentation labelì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ê²ƒì´ë‹¤.

ê·¸ëŸ°ë°, **í›ˆë ¨ ì´ë¯¸ì§€ê°€ ìˆëŠ” ìƒí™©**ì—ì„  í›„ì²˜ë¦¬ íš¨ê³¼ê°€ ë” ê°ì†Œí•˜ëŠ” ê²ƒì„ ë°œê²¬í•˜ì˜€ë‹¤. ë”°ë¼ì„œ image-freeì— ëŒ€í•´ì„œë§Œ í›„ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ í•˜ì˜€ë‹¤.

<br>

## ğŸ§© Related Works
### Transferable image segmentation
ì—¬ì „íˆ ìƒˆë¡œìš´ visual categoryë“¤ì„ ë¶„í• í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤. ì´ë¥¼ ìœ„í•´ unsupervised ë° zero-shot segmentation ë°©ë²•ë“¤ì´ ì‹œë„ë˜ì—ˆë‹¤.

- **unsupervised ì ‘ê·¼ë²•** : ì´ë¯¸ì§€ì˜ ë°€ì§‘í•œ representationì„ í´ëŸ¬ìŠ¤í„°ë§í•œ í›„, í—ê°€ë¦¬ì•ˆ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ **ë¶„í•  ì¹´í…Œê³ ë¦¬ë¥¼ ë§¤ì¹­**í•œë‹¤.
- **zero-shot ì´ˆê¸° ì ‘ê·¼ë²•** : ë‹¨ì–´ ì„ë² ë”©ì„ í†µí•œ ë¶„í•  ì–´íœ˜ í™œìš©.  classì— ë¬´ê´€í•œ semantic maskë‚˜ í´ë˜ìŠ¤ë³„ segmentation annotationì„ í•„ìš”ë¡œ í•œë‹¤

â‡’ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¯¸ì§€ë‚˜ ê¸°íƒ€ ì£¼ì„ë³´ë‹¤ ì‰½ê²Œ ìˆ˜ì§‘í•  ìˆ˜ ìˆëŠ” **semantic categoryë§Œ ì£¼ì–´ì§„, ì´ë¯¸ì§€ ì—†ëŠ” ì‹œë‚˜ë¦¬ì˜¤**ì—ì„œ í•  ìˆ˜ ìˆë„ë¡ í•¨.

<br>

## ğŸ§© Experiment
### 1. Image-Free Adaptation for Segmentation
#### â¡ï¸ 1.1. Zero-shot image segmentation
**unseen category**ì— ëŒ€í•´ image segmentationì„ ì§„í–‰í•˜ê³ , **mIOU ì ìˆ˜**ë¥¼ ë¹„êµí•˜ì—¬ í‰ê°€ë¥¼ ì§„í–‰í•œë‹¤.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/479e84eb-8a9f-49ec-a7bd-902d47d063a8" width="60%"></center>

- **ì•Œì•„ë‘˜ ê²ƒ**
  - **MaskCLIP+** : MaskCLIPì— ì˜í•´ ìƒì„±ëœ pseudo labelì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ì ì¸ ë°ì´í„°ë¥¼ ë” í›ˆë ¨í•œ ëª¨ë¸
- **ê²°ê³¼**
  - ê¸°ì¡´ **baselineë³´ë‹¤ ì„±ëŠ¥**ì´ ìƒë‹¹ì´ í–¥ìƒë˜ì—ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ ì¤‘ ì„±ëŠ¥ì´ ê°€ì¥ ë†’ì€ MaskCLIPë³´ë‹¤ 30.8 point ë†’ì€ ì„±ëŠ¥ì„ ì–»ì—ˆë‹¤.
  - semanntic categoryë¥¼ ì œì™¸í•˜ê³  **ì–´ë–¤ imageë‚˜ annotationì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì²´ì œ**ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì¶”ê°€ì ì¸ ì´ë¯¸ì§€ë“¤ì— ëŒ€í•´ í›ˆë ¨ëœ ë³´ë‹¤ ê°•ë ¥í•œ **MaskCLIP+ë¥¼ ëŠ¥ê°€**í•œë‹¤.

<br>

#### â¡ï¸ 1.2. Cross-Dataset Transfer
COCO Stuffì—ì„œ í›ˆë ¨ë˜ê³ , ADE20K ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‰ê°€í•˜ì—¬ **Dataset êµì°¨ ì „ì´ ì„±ëŠ¥**ì„ íŒŒì•…í•œë‹¤.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/9c39e057-3ebd-46e9-bfc9-6e42c47b77cd" width="80%"></center>

- **ì•Œì•„ë‘˜ ê²ƒ** : â€  ëŠ” **í›„ì²˜ë¦¬(post-processing)**ê°€ ì ìš©ëœ ê²ƒì„ ì˜ë¯¸í•œë‹¤.
- **ê²°ê³¼**
  - í›ˆë ¨ëœ imageì™€ classì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” **semantic mask annotation**ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ëœ **OpenSeg**ë³´ë‹¤ 1.5 point ë†’ì€ ì„±ëŠ¥ì„ ì–»ì—ˆë‹¤. <font style="color:yellow">â– </font>
  - ZSSegë³´ë‹¤ ë‚®ì€ mIOUë¥¼ ì–»ì—ˆì§€ë§Œ, **í›ˆë ¨ ê·œëª¨ ì‚¬ì´**ì—ëŠ” ì°¨ì´ê°€ í¬ë‹¤. <font style="color:green">â– </font>
  - í›„ì²˜ë¦¬ë¥¼ ì ìš©í•œ baselineë³´ë‹¤ **IFSeg + í›„ì²˜ë¦¬**ì˜ mIOUê°€ 6.5~15.7ë§Œí¼ ë” ë†’ë‹¤.

<br>

#### â¡ï¸ 1.3. Unsupervised Image Segmentation
**annotationì´ ì—†ëŠ” ìƒíƒœ**ì—ì„œ ì§„í–‰í–ˆì„ ë•Œ, ì„±ëŠ¥ì„ ë¹„êµí•œë‹¤.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/cfd97de8-53c7-4f65-b988-c128c37e3d79" width="60%"></center>

- **ì•Œì•„ë‘˜ ê²ƒ** : â€  ëŠ” **í›„ì²˜ë¦¬(post-processing)**ê°€ ì ìš©ëœ ê²ƒì„ ì˜ë¯¸í•œë‹¤.
- **ê²°ê³¼**
  - ê¸°ì¡´ì˜ image-free segment **baseline**ì„ ëŠ¥ê°€í•œë‹¤.
  - ì¶”ê°€ í›ˆë ¨ì´ ì§„í–‰ëœ MaskCLIP+ì— ëŒ€í•´ì„œ 1.1 point ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.
    ê·¸ëŸ¬ë‚˜ **í›ˆë ¨ ê·œëª¨ ì°¨ì´**ë¥¼ ìƒê°í–ˆì„ ë•Œ íš¨ìœ¨ì ì¸ ê²ƒì„ ì§ì‘í•  ìˆ˜ ìˆë‹¤.

<br>

#### **â¡ï¸ 1.4. Qualitive Results**
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/8954e0bc-7e9c-4bc8-9038-dc3f641fa71b"></center>

- ê¸°ì¡´ segment ê¸°ìˆ ë³´ë‹¤ ë” **ì •êµí•˜ê²Œ segment**ê°€ ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
- **ground truth label**ë³´ë‹¤ **ë” ì„¸ë°€í•œ category**ê¹Œì§€ segmentí•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

<br>

### 2. Ablation Study
**training imageê°€ ìˆê³ , segmentation annotationì´ ìˆëŠ”** ê²½ìš°ì—”, ì–¼ë§ˆë‚˜ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ì§€ í™•ì¸í•œ ì‹¤í—˜ì´ë‹¤.

#### â¡ï¸ 2.1. Self-training
seen categoryì™€ unseen category ì‚¬ì´ì˜ ê²©ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•œ semi-supervised ë°©ì‹ìœ¼ë¡œ, **pseudo label**ì„ ìƒì„±í•˜ëŠ” ë°©ì‹ì´ë‹¤. 
ë³¸ ì‹¤í—˜ì—ì„œëŠ” training imageì™€ seen annotationì´ ìˆë‹¤ëŠ” ê°€ì • í•˜ì— ì§„í–‰ì„ í•œë‹¤.

- **ê³¼ì •**
  - IFSegë¥¼ 118k ì´ë¯¸ì§€ì™€ seen annotationì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ë¡œ fine-tuningí•œë‹¤. ê·¸ í›„ unseen categoryì— ëŒ€í•´ í‰ê°€ë¥¼ í•œë‹¤.
- **ê²°ê³¼**
  - ê¸°ì¡´ ë°©ë²•ì˜ ê²°ê³¼ì—ˆë˜ 55.6ì—ì„œ 61.6ìœ¼ë¡œ í¬ê²Œ í–¥ìƒ ë˜ì—ˆë‹¤.
  <center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/3fe0c5fd-17a0-4f3c-ae42-86100a17fbe0" width="60%"></center>
  - ë˜í•œ **MaskCLIP+**ì— ëŒ€í•´ì„œë„ í¬ê²Œ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë³´ì˜€ë‹¤.
  <center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/7670f3be-01f5-43fb-a47b-7546082bce77" width="55%"></center>

<br>


#### â¡ï¸ 2.2. Supervised Semantic Segmentation
supervised learningì„ ì‚¬ìš©í•˜ì—¬ semantic segmentationì„ ì§„í–‰í•œë‹¤. 
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/0ec814c1-c0ad-46ea-8d89-962c93fe01b5" width="60%"></center>

- **ê²°ê³¼** : ADE20k ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê°€ì¥ ê°•ë ¥í–ˆë˜ ê¸°ì¡´ì˜ ë°©ì‹ë³´ë‹¤ 2.0 point ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

<br>

## ğŸ§© Supplementary Matrerial
ë…¼ë¬¸ 12í˜ì´ì§€ë¶€í„° ì¶”ê°€ì ì¸ ì‹¤í—˜ì— ëŒ€í•´ ì–¸ê¸‰ë˜ì–´ ìˆìœ¼ë©°, parameter ê°’ì„ ì§€ì •í•˜ëŠ” ì—¬ëŸ¬ ì‹¤í—˜ì— ëŒ€í•´ ê¸°ì¬ë˜ì–´ ìˆë‹¤. ì•„ì£¼ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•˜ë©´ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
- **Post processing**ì—ì„œ **K-nearest neighbor** ë°©ë²•ì„ ì‚¬ìš©í•  ë•Œ, ë°˜ë³µ ìˆ˜ë¥¼ 25 / Kë¥¼ 3ìœ¼ë¡œ ì§€ì •í–ˆë‹¤. ë°˜ë³µìˆ˜ë¥¼ 25ë¡œ ì •í•œ ì´ìœ ëŠ” ê°’ì„ 0ë¶€í„° 50ê¹Œì§€ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë ¸ì„ ë•Œ, 24ì™€ 50ì´ ê°€ì¥ í° mIOUë¥¼ ë³´ì˜€ê³ , ë‘ ê°’ì˜ ì°¨ì´ê°€ ë¯¸ë¯¸í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ë˜í•œ Kê°€ ë†’ì•„ì§ˆ ìˆ˜ë¡ mIOUê°€ ë†’ì•„ì§„ë‹¤.
- **Post processing**ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•˜ê¸° ìœ„í•œ ì‹¤í—˜ì„ ì§„í–‰í–ˆìœ¼ë©°, í›„ì²˜ë¦¬ê°€ ì ìš©ëœ ê²ƒì´ ëª¨ë‘ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.
  <center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/2f0e661c-2440-47b9-a75d-e9c4c49c2145" width="60%"></center>
- cross-attention mechanismì„ ì—†ì• ë©´ ì„±ëŠ¥ì´ ê°ì†Œí•œë‹¤.
- ViT-B/16ì´ ResNetë³´ë‹¤ visual representationì´ ë” í–¥ìƒë˜ì—ˆìœ¼ë‚˜, IFSegë¥¼ ResNetê³¼ ì‚¬ìš©í•œ ê²ƒì´ ViT Backboneì„ ì‚¬ìš©í–ˆì„ ë•Œë³´ë‹¤ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

<br>

## ğŸ§© Conclusion
- target semantic categoryë¥¼ ì œì™¸í•˜ê³ , ì–´ë– í•œ **task-specific image**ë‚˜ **annotation ì—†ì´** **semantic segmentation**ì„ ìˆ˜í–‰í•˜ì˜€ë‹¤.
- í•µì‹¬ ì•„ì´ë””ì–´ëŠ” semantic category wordë“¤ì´ pre-trained VL modelì˜ cross modal embedding spaceì—ì„œ **artificial image token**ìœ¼ë¡œ ì‘ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.
- stronger supervisionì„ ëŠ¥ê°€í•˜ëŠ” ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

<br>

## ğŸ§© My Opinion
Qualitive Results ë¶€ë¶„ë§Œ ë´ë„, segmentation ìì²´ì˜ ì„±ëŠ¥ì´ ë§¤ìš° í–¥ìƒëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆëŠ”ë°, ì´ì— ë”ë¶ˆì–´ ground truthë³´ë‹¤ ë” ì •êµí•œ labelë¡œ í•´ë‹¹ taskë¥¼ ì§„í–‰í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ë³¸ ë…¼ë¬¸ì„ ì½ê²Œ ëœ ê°€ì¥ í° ë™ê¸°ê°€ ë˜ì—ˆë˜ ê²ƒ ê°™ë‹¤. ì§€ì¸ ë¶„ê»˜ì„œ Meta AIì˜ **Segment Anything Model(SAM)**ì™€ ë¹„ìŠ·í•œ ê²ƒì´ëƒê³  ì—¬ì­¤ë³´ì…”ì„œ ì¢€ ë” íŒŒì•…ì„ í•´ë³´ì•˜ëŠ”ë°, **ì°¨ì´ì **ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/6141e811-7cc2-4198-8a55-5675322df3e3" width="100%"></center>
outputì— ë”°ë¼ ìì‹ ì´ ì›í•˜ëŠ” ê¸°ëŠ¥ì„ í•˜ëŠ” ê¸°ìˆ ì„ ì‚¬ìš©í•˜ë©´ ë  ê²ƒ ê°™ë‹¤.

<br>

## ğŸ§© Reference
- **IFSeg Github(paper link is in here)** : [https://github.com/alinlab/ifseg](https://github.com/alinlab/ifseg)
- **SAM Website** : [https://segment-anything.com/](https://segment-anything.com/)