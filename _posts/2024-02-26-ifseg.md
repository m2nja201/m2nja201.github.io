---
title:  "[논문 리뷰] IFSeg :: Image-Free Semantic Segmentation via Vision-Language Model"
excerpt: "Image-Free Semantic Segmentation via Vision-Language Model (IFSeg) 논문 리뷰"

toc: true
toc_sticky: true

categories:
  - ComputerVision 
  - PaperReview
  - MultiModal

use_math: true
header:
  teaser: https://github.com/m2nja201/m2nja201.github.io/assets/80443295/0bb5f9f3-9882-4f88-a279-dd54dd978836

---

논문 <font style="color:hsl(27, 100%, 43%)">Image-Free Semantic Segmentation via Vision-Language Model</font>을 읽고, 
최대한 다른 분들이 보셨을 때 이해가 잘 되도록 요약 및 핵심 내용 설명 위주로 Paper Review를 진행할 예정입니다.

논문에서의 순서를 일정하게 지키는 것보다, 각 목차에서 필요할 것 같다 생각하는 부분을 먼저 언급하거나, 뒷 부분에서 언급할 수 있습니다. 보시고 피드백도 자유롭게 주셨으면 좋겠습니다!

## 🧩 Abstract & Introduction
- 새로운 개념들에 대해 VL model pre-training이 각광 받고 있지만, **segmentation**에 있어선 많이 탐색되지 않았다.
- downstream task에 적응시킬 때 추가적인 **training image**나 **segmentation annotation**이 필요하다는 문제점이 있다.
    — 웹 이미지와 같이 task specific training image나 label이 없는 경우가 많기 때문에 데이터를 쉽게 구할 수 없다.

### Their Approach

⭐ **target semantic category**만을 사용하여 어떠한 task specific image와 annotation 없이 **semantic segmentation**을 진행하는 것 ⇒ **Image-Free** Semantic Segmentation [**IFSeg**]

아래 사진은 category의 종류만 prompt로 주어지고, 웹에서 다운 받은 이미지이기 때문에 아무런 정보가 없음에도 불구하고 다음과 같이 Segmentation 성능이 높다.

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/8376b58e-a378-4bed-b425-1defa638fc59" width="50%"></center>


**방법**은 다음과 같다.

1. **word token**을 사용하여 VL 기반의 image-segmentation 쌍을 **인공적**으로 생성
        
    > 🔍 **인공적으로 생성할 수 있는 이유** : pre-trained VL model은 visual token과 text token이 공통 space에서 **가깝게** 위치하기 때문에, artificial word map이 실제 이미지 input을 대체할 수 있다.
    
    즉, **semantic category의 단어 세트**가 embedding space에서 VL 모델에 대한 **artificial image**로 기능할 수 있다는 것
        
2. segmentation task로 VL model 업데이트

### Contributions

- target semantic category만 주어지고, 어떠한 task specific image와 annotation 없을 때, 본 category를 세분화하는 **IFSeg**를 소개하였다.
- 오직 semantic segmentation category만 주어진 **weaker supervision**을 사용함에도 불구하고, 기존 baseline들을 능가할 수 있다.
- COCO Stuff, ADE20K 벤치마크에서 성능과 훈련 용량을 비교하여 효율적인 성능을 도출함을 보여주었다.

<br>

## 🧩 Method
**Base** : **self-supervised** 방식, VL encoder-decoder model + IFSeg 모델, semantic segmentation 진행

### 1. VL Encoder-Decoder Architecture
#### ➡️ 1.1. Data Format
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/68f2be96-d7b6-40fa-9faf-9cc51379d904" width="60%"></center>
sequence data를 기반으로 작동한다. (**x** : sequence data, **$$L_x$$** : length of sequence data, **$$e_x$$** : D차원 vector 공간에서의 embedding)

1. raw image-text($$χ_I, χ_T$$)를 토큰의 **sequence**로 tokenizing하여 처리한다.
  - 이때, $$ χ_T $$는 사전에 정의된 N개의 단어 **dictionary** $$ V = {(v_0,..., v_{N-1})} $$에의해 tokeninzing 됨.
  - $$χ_I$$는 image backbone($$f_{img}$$)이 도입되어 변형된 후, 출력 사이즈에 맞게 linear layer가 적용된다.
  <center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/ce817efe-7ba3-4bd9-847c-ac74115f76a9" width="50%"></center>
2. $$χ_I, χ_T$$를 결합한다.

<br>

#### ➡️ 1.2. VL Model architecture
multi modal source **`x`**가 주어졌을 때, 학습된 분포를 기반으로 target **`y`**를 예측한다.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/87ce94cb-a5ae-4dff-84fd-00f722f50e77" width="60%"></center>

- **Encoder : $$f_{enc}$$**는 **self-attention** mechanism을 통해 $$e_x$$를 $$x$$의 embedding으로 제공한다. 즉, **self-attention**을 통해 각 단어가 문장의 다른 부분과 상호작용할 수 있도록 한다.
- **Decoder : $$f_{dec}$$**는 **self-attention**과 **cross-attention**를 통해 encoder의 출력과 decoder에서의 상태를 주목한다.

<br>

### 2. Semantic Segmentation via Encoder-Decoder
> 🔍 **Semantic Segmentation** : 관심 있는 M개의 **Semantic category**들이 주어졌을 때, 각 이미지의 **영역**에 대한 **category word**를 decoding하는 것

그러나, 특정 semantic category (ex : `giraffe`) 가 dictionary V에서 **다중의 subword** (ex : `_gir`, `affe`)로 토큰화될 수 있기 때문에 번거롭다.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/2b84a06f-7a8c-47fe-a789-bfce5b7c0ec4"></center>

<br>

#### ➡️ 2.1. 위 문제점 해결 방법
- `giraffe`와 같은 category들을 **임시 추가 단어**로 처리한 뒤, **sub word token들의 평균 임베딩**을 임베딩 행렬 E에 추가한다.
- 그러고 난 뒤, 각 카테고리들은 하나의 word로 인식되게 된다. 이때 최종 범주 집합은 다음과 같다. $$ V_{seg} = $$ $$\{v^{'}_0, ... , v^{'}_{M-1}\} $$

<br>

#### ➡️ 2.2. Prompt Design
text token xT는 semantic segmentation 작업의 세부사항을 지시하기 위한 prompt로 제공될 수 있다. 즉 **작업 설명**과 **target class의 목록**이다. 이때 VQA 작업에서 ‘task description + category enumeration’ protocol을 따른다.

> **🔍 VQA (Visuial Question Answering)** 
- **작업 설명** : ‘What is the segmentation map of the image?” 같은 것 . (수행해야할 작업 종류)
- **카테고리 나열** :  대상이 되는 카테고리들을 나열 . “object : giraffe, grass”와 같이

<br>

### 3. Image-Free Semantic Segmentation
⭐ **Main Idea** : VL pretraining 동안 실제 이미지 token과, 상응되는 **semantic category word token**은 공유된 임베딩 공간에서 **근접하게 위치할 가능성**이 높기 때문에 다음 내용이 가능

- 주어진 단어 토큰들을 사용하여 **artificial image token**을 생성한다.
- VL 모델을 업데이트하여 해당 단어 토큰들을 self-supervised 방식으로 segmentation을 진행한다.
> 🔍 **Artificial Image Token** : image와 segmentation label을 쌍으로 하는 학습 데이터

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/8daf2ab4-8f71-4330-b7ac-bb5ebe7d66d7"></center>

<br>

#### ➡️ 3.1. Constructing Artificial Image Token
M개의 category 단어 집합 **$$V_{seg}$$**에서 **artificial training data**(image-segmentation token 쌍)을 구성하고, 단어들은 무작위로 선택되어 **grid map**을 형성하는 데에 사용된다.

- 초기 Grid size는 정해진 범위 내에서 **무작위**로 결정되며, 이후 이미지 공간 해상도(**H*W**)에 맞게 조정된다.
- 이때, **이웃 보간법(nearest neighbor)**이 사용된다. 이는 실제 이미지 데이터 없이도 학습할 수 있도록 한다.
    
    $$
    v_{IFSeg} = [V^0_{IFSeg}, ... , v^{(H*W-1)}_{IFSeg}]
    $$
    
- 다양한 **random map**을 사용하여 데이터를 **up-sample** 하는 것이 목표이다.

> 🔍 **Grid map** :  image의 segmentation을 학습하기 위한 **artificial token**의 배열

<br>

#### ➡️ 3.2. Post-processing for image-free segmentation [후처리]
실제 이미지 데이터가 없는 상황에서 발생하는 **train data와 evalutation data 간의 차이**를 극복하기 위한 방법을 제시한다.

- image backbone을 기반으로 한 **출력 확률값을 평균** 내는 것이 segmentation 품질을 향상시킨다는 것을 발견하였다.
    - cosine 유사도를 사용하여 이미지 특성의 **K-nearest neighbor** 탐색을 한다.
    - 이들의 **확률값을 평균**하여, 각 픽셀에 대한 segmentation label의 정확도를 높이는 것이다.

그런데, **훈련 이미지가 있는 상황**에선 후처리 효과가 더 감소하는 것을 발견하였다. 따라서 image-free에 대해서만 후처리를 사용하기로 하였다.

<br>

## 🧩 Related Works
### Transferable image segmentation
여전히 새로운 visual category들을 분할하는 것은 어렵다. 이를 위해 unsupervised 및 zero-shot segmentation 방법들이 시도되었다.

- **unsupervised 접근법** : 이미지의 밀집한 representation을 클러스터링한 후, 헝가리안 매칭 알고리즘을 통해 **분할 카테고리를 매칭**한다.
- **zero-shot 초기 접근법** : 단어 임베딩을 통한 분할 어휘 활용.  class에 무관한 semantic mask나 클래스별 segmentation annotation을 필요로 한다

⇒ 본 논문에서는 이미지나 기타 주석보다 쉽게 수집할 수 있는 **semantic category만 주어진, 이미지 없는 시나리오**에서 할 수 있도록 함.

<br>

## 🧩 Experiment
### 1. Image-Free Adaptation for Segmentation
#### ➡️ 1.1. Zero-shot image segmentation
**unseen category**에 대해 image segmentation을 진행하고, **mIOU 점수**를 비교하여 평가를 진행한다.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/479e84eb-8a9f-49ec-a7bd-902d47d063a8" width="60%"></center>

- **알아둘 것**
  - **MaskCLIP+** : MaskCLIP에 의해 생성된 pseudo label을 사용하여 추가적인 데이터를 더 훈련한 모델
- **결과**
  - 기존 **baseline보다 성능**이 상당이 향상되었다. 기존 모델 중 성능이 가장 높은 MaskCLIP보다 30.8 point 높은 성능을 얻었다.
  - semanntic category를 제외하고 **어떤 image나 annotation을 사용하지 않는 체제**에도 불구하고, 추가적인 이미지들에 대해 훈련된 보다 강력한 **MaskCLIP+를 능가**한다.

<br>

#### ➡️ 1.2. Cross-Dataset Transfer
COCO Stuff에서 훈련되고, ADE20K 벤치마크에서 평가하여 **Dataset 교차 전이 성능**을 파악한다.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/9c39e057-3ebd-46e9-bfc9-6e42c47b77cd" width="80%"></center>

- **알아둘 것** : † 는 **후처리(post-processing)**가 적용된 것을 의미한다.
- **결과**
  - 훈련된 image와 class에 구애받지 않는 **semantic mask annotation**을 사용하여 훈련된 **OpenSeg**보다 1.5 point 높은 성능을 얻었다. <font style="color:yellow">■</font>
  - ZSSeg보다 낮은 mIOU를 얻었지만, **훈련 규모 사이**에는 차이가 크다. <font style="color:green">■</font>
  - 후처리를 적용한 baseline보다 **IFSeg + 후처리**의 mIOU가 6.5~15.7만큼 더 높다.

<br>

#### ➡️ 1.3. Unsupervised Image Segmentation
**annotation이 없는 상태**에서 진행했을 때, 성능을 비교한다.
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/cfd97de8-53c7-4f65-b988-c128c37e3d79" width="60%"></center>

- **알아둘 것** : † 는 **후처리(post-processing)**가 적용된 것을 의미한다.
- **결과**
  - 기존의 image-free segment **baseline**을 능가한다.
  - 추가 훈련이 진행된 MaskCLIP+에 대해서 1.1 point 낮은 성능을 보였다.
    그러나 **훈련 규모 차이**를 생각했을 때 효율적인 것을 짐작할 수 있다.

<br>

#### **➡️ 1.4. Qualitive Results**
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/8954e0bc-7e9c-4bc8-9038-dc3f641fa71b"></center>

- 기존 segment 기술보다 더 **정교하게 segment**가 되는 것을 확인할 수 있다.
- **ground truth label**보다 **더 세밀한 category**까지 segment한다는 것을 알 수 있다.

<br>

### 2. Ablation Study
**training image가 있고, segmentation annotation이 있는** 경우엔, 얼마나 더 좋은 성능을 보이는지 확인한 실험이다.

#### ➡️ 2.1. Self-training
seen category와 unseen category 사이의 격차를 줄이기 위한 semi-supervised 방식으로, **pseudo label**을 생성하는 방식이다. 
본 실험에서는 training image와 seen annotation이 있다는 가정 하에 진행을 한다.

- **과정**
  - IFSeg를 118k 이미지와 seen annotation을 사용하여 추가로 fine-tuning한다. 그 후 unseen category에 대해 평가를 한다.
- **결과**
  - 기존 방법의 결과었던 55.6에서 61.6으로 크게 향상 되었다.
  <center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/3fe0c5fd-17a0-4f3c-ae42-86100a17fbe0" width="60%"></center>
  - 또한 **MaskCLIP+**에 대해서도 크게 증가하는 것을 보였다.
  <center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/7670f3be-01f5-43fb-a47b-7546082bce77" width="55%"></center>

<br>


#### ➡️ 2.2. Supervised Semantic Segmentation
supervised learning을 사용하여 semantic segmentation을 진행한다. 
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/0ec814c1-c0ad-46ea-8d89-962c93fe01b5" width="60%"></center>

- **결과** : ADE20k 벤치마크에서 가장 강력했던 기존의 방식보다 2.0 point 높은 성능을 보인다.

<br>

## 🧩 Supplementary Matrerial
논문 12페이지부터 추가적인 실험에 대해 언급되어 있으며, parameter 값을 지정하는 여러 실험에 대해 기재되어 있다. 아주 간단하게 요약하면 결과는 다음과 같다.
- **Post processing**에서 **K-nearest neighbor** 방법을 사용할 때, 반복 수를 25 / K를 3으로 지정했다. 반복수를 25로 정한 이유는 값을 0부터 50까지 점진적으로 늘렸을 때, 24와 50이 가장 큰 mIOU를 보였고, 두 값의 차이가 미미했기 때문이다. 또한 K가 높아질 수록 mIOU가 높아진다.
- **Post processing**의 효과를 입증하기 위한 실험을 진행했으며, 후처리가 적용된 것이 모두 더 높은 성능을 보였다.
  <center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/2f0e661c-2440-47b9-a75d-e9c4c49c2145" width="60%"></center>
- cross-attention mechanism을 없애면 성능이 감소한다.
- ViT-B/16이 ResNet보다 visual representation이 더 향상되었으나, IFSeg를 ResNet과 사용한 것이 ViT Backbone을 사용했을 때보다 더 높은 성능을 보였다.

<br>

## 🧩 Conclusion
- target semantic category를 제외하고, 어떠한 **task-specific image**나 **annotation 없이** **semantic segmentation**을 수행하였다.
- 핵심 아이디어는 semantic category word들이 pre-trained VL model의 cross modal embedding space에서 **artificial image token**으로 작용할 수 있다는 것이다.
- stronger supervision을 능가하는 강력한 성능을 보였다.

<br>

## 🧩 My Opinion
Qualitive Results 부분만 봐도, segmentation 자체의 성능이 매우 향상된 것을 확인할 수 있는데, 이에 더불어 ground truth보다 더 정교한 label로 해당 task를 진행할 수 있다는 것이 본 논문을 읽게 된 가장 큰 동기가 되었던 것 같다. 지인 분께서 Meta AI의 **Segment Anything Model(SAM)**와 비슷한 것이냐고 여쭤보셔서 좀 더 파악을 해보았는데, **차이점**은 다음과 같다. 

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/6141e811-7cc2-4198-8a55-5675322df3e3" width="100%"></center>
output에 따라 자신이 원하는 기능을 하는 기술을 사용하면 될 것 같다.

<br>

## 🧩 Reference
- **IFSeg Github(paper link is in here)** : [https://github.com/alinlab/ifseg](https://github.com/alinlab/ifseg)
- **SAM Website** : [https://segment-anything.com/](https://segment-anything.com/)