---
title:  "[Paper Review] CoOp :: Learning to Prompt for Vision-Language Models"
excerpt: "Learning to Prompt for Vision-Language Models (CoOp) 논문 리뷰"

toc: true
toc_sticky: true

categories:
  - ComputerVision 
  - PaperReview
  - MultiModal

---

논문 <font style="color:hsl(27, 100%, 43%)">Learning to Prompt for Vision-Language Models</font>을 읽고, 
최대한 다른 분들이 보셨을 때 이해가 잘 되도록 요약 및 핵심 내용 설명 위주로 Paper Review를 진행할 예정입니다.

보시고 피드백도 자유롭게 주셨으면 좋겠습니다!

## 🦾 Abstract
- 이전의 전통적인 방식들 : discrete한 label을 사용 (class를 label로 매핑)
- CLIP과 같은 대용량 <font style="color:hsl(27, 100%, 43%)">pre-trained VL Model</font> : Image와 Text를 공통 Feature Space에 정렬하고, <font style="color:hsl(27, 100%, 43%)">prompting</font>을 통해 downstream task에 대한 즉시 전이를 가능하게 함

그러나 이러한 모델들의 가장 큰  <font style="color:hsl(27, 100%, 43%)">Challenge</font>는 다음과 같다.
1. Domain에 대한 전문 지식이 필요한 prompt engineering을 해야한다. 즉, task에 따라 또 다시 prompt engineering을 해야한다는 의미이다.
2. 이에 따른 시간 소모가 크다.
3. 미세한 단어 선택이 성능에 큰 영향을 준다.

### 그래서 제안한 것이  <strong style="color:hsl(27, 100%, 43%)">Context Optimization [CoOp]</strong>이다. 
> 이는 CLIP과 같은 **pre-trained VL model**에 적용시켜서, image recognition과 같은 **downstream task**에 잘 적응하도록 한다.

<center>
<img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/51cfaff0-95e2-4a4b-98ce-29884b521754" width='50%' height='50%' /> </center>


<strong style="color:hsl(27, 100%, 43%)">CoOp</strong>을 아주 간단히 설명하자면, 학습 가능한 vector로 prompt의 context word를 구성하고, 이때 pre-trained parameter는 고정시킨다.

### CoOp's Contribution 요약
- 손으로 만든 prompts (<font style="color:hsl(27, 100%, 43%)">hand-crafted prompts</font>) 를 one/two shots으로 이길 수 있다.
- few shots을 사용했을 때는 다른 prompt enginnering 기법보다 15~45% 성능이 좋았다.
- learning 기반 접근 방식임에도 불구하고, 직접 만든 prompts를 사용한 zero-shot model에 비해 더 뛰어난 <font style="color:hsl(27, 100%, 43%)">도메인 일반화 성능</font>을 보인다.

<br>

## 🦾 Introduction
### 기존의 연구
discrete label을 이용하여 <font style="color:hsl(27, 100%, 43%)">fixed set of object categories</font>를 예측했었다.
즉, 카테고리의 text들은 인덱싱을 거쳐, discrete label로 된 것이다.

예를 들어 ``gold fish``와 ``toilet paper``와 같은 단어들은 단지, <font style="color:hsl(27, 100%, 43%)">cross-entropy loss</font>를 용이하게 하기 위한 discrete label로 분류될 뿐, text에 <font style="color:hsl(27, 100%, 43%)">내포된 의미</font>를 직접적으로 사용한 것이 아니다.

> 즉, 기존에 사용되는 label들은 **text의 의미**를 이미지 특성과 매칭시킬 수 없다는 것이다.

이러한 학습 방법은 visual recognition system을 <font style="color:hsl(27, 100%, 43%)">폐쇠된 시각적 개념</font>에 한정시키고, <font style="color:hsl(27, 100%, 43%)">새로운 categories</font>에 대해 다룰 수 없도록 한다.

<hr>

### Vision-Language Learning
그래서 CLIP과 같은 vision-language 모델이 시각 표현 학습의 대안으로 사용되었다.

주요 아이디어는 <font style="color:hsl(27, 100%, 43%)">Image Encoder</font>와 <font style="color:hsl(27, 100%, 43%)">Text Encode</font> 총 두 개의 인코더에서 image와 raw text를 정렬하는 것이다. 이 모델들은 image-text pair에 대해 **positive**이면 feature space 상에서 가까워지도록 하고, **negative pair**이면 멀어지도록 학습한다.

<center>
<img src="https://miro.medium.com/v2/resize:fit:3662/1*tg7akErlMSyCLQxrMtQIYw.png" width='70%' height='70%' /> </center>


<hr>

### 기존 VL model에서의 한계점
그러나 Abstract에서 언급했듯이, 한계점이 존재한다.
- word tuning을 하는 데에는 상당한 시간이 소모된다.
- 단어 구조의 작은 변경으로도 성능에 큰 영향을 끼친다. 아래 사진과 같이 ``photo``라는 단어 추가에 약 5% 성능이 향상되고, ``[CLASS]``의 위치에도 큰 영향을 받는 것을 볼 수 있다.

<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/22dd311a-3d7a-4404-9429-da0a81607421" width='50%' height='50%' /> </center>

- task에 대한 사전 지식과, 언어 모델에 대한 지식이 필요하다.

<hr>

### CoOp
NLP에서 영감을 받아 <font style="color:hsl(27, 100%, 43%)">prompt engineering</font>을 자동화 하는 접근 방식으로 <font style="color:hsl(27, 100%, 43%)">CoOp</font>을 제안하였다.

※ CoOp의 Method에 대해서는 너무 여러 번 언급되는 감이 있어 Method 부분에서 다루도록 하겠습니다!

<br>

## 🦾 Contributions
text prompt를 discrete category로 생각 (기존의 prompt engineering) 하지 않고, <font style="color:hsl(27, 100%, 43%)">continuous signal</font>로 취급하였다.
- 기존의 VLP 방식에서의 비효율적인 문제점들을 제시하였다.
- pre-trained VL model을 통해 prompt engineering이 자동화될 수 있도록, <font style="color:hsl(27, 100%, 43%)">continous prompt learning</font> 방식으로 접근하였다.
- 구현 방식을 <font style="color:hsl(27, 100%, 43%)">unified / class specific</font> 두 가지로 제시함으로써 다양한 recognition task에 적용될 수 있는 방법을 제시하였다.
- hand crafted prompt 방식과, linear probe model 방식에 비해 더 효율적으로 <font style="color:hsl(27, 100%, 43%)">최적화</font>가 가능하며, <font style="color:hsl(27, 100%, 43%)">성능</font>이 현저히 높아졌으며, <font style="color:hsl(27, 100%, 43%)">domain shift</font>에 보다 더 robust 하다.

<br>

## 🦾 Related Works
※ 논문의 내용을 자세히 나열하기 보다, 필요한 부분만 작성하였습니다.

### Vision Language Model
이 논문에서의 목표는 VL model에 대한 연구라기 보다, 이 모델들을 <font style="color:hsl(27, 100%, 43%)">downstream task에 적응시키고 전이하는 것</font>을 효율화 시키는 것에 중점을 두고 있다.

<hr>

### Prompt Learning in NLP
- probing : 주어진 cloze-style에 대해 정답을 생성하게 하는 것
- knowledge probing : ``fill-in-the-blank``(빈칸 채우기) 방식의 cloze texts 방법이 제시되었다.

본 논문에서 채택한 prompt learning 방식은 <font style="color:hsl(27, 100%, 43%)">continous prompt learning</font>이다. 이는, word embedding space에서 <font style="color:hsl(27, 100%, 43%)">continous</font>한 벡터를 사용하는 방식이다.

하지만 이 방식은 문제점이 있다. 학습된 word가 <font style="color:hsl(27, 100%, 43%)">정확히 어떤 prompt를 나타내는지 시각화</font>를 할 수 없다.

그래도 이 방식을 채택한 이유는 prompt embedding을 추출하는 것이 목적이 아닌, 
VL model을 <font style="color:hsl(27, 100%, 43%)">downstream task</font>에 적용하여 성능이 좋고, <font style="color:hsl(27, 100%, 43%)">domain shift</font>에 강인한 prompt를 <font style="color:hsl(27, 100%, 43%)">자동화</font>하기 위함이다.

<br>

## 🦾 Method

### [1] Vision-Language Pre-training
<center><img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/7e6140bd-2579-4167-bb71-a82e42b9dda8" width='70%' height='70%' /> </center>

#### ➡️ Models
두 개의 인코더를 가지고 있고, 하나는 image, 하나는 text encoder이다. <font style="color:hsl(27, 100%, 43%)">CLIP baseline</font>이다.
- <font style="color:hsl(27, 100%, 43%)">Image Encoder</font>
  - high-dimensional image를 low-dimensional embedding space로 mapping한다.
  - ResNet50의 CNN과 ViT의 transformer baseline처럼 사용하였다.
- <font style="color:hsl(27, 100%, 43%)">Text Encoder</font> : transformer

#### ➡️ Encoding 방식
각 text sequence는 ``[SOS]``, ``[EOS]`` 토큰으로 둘러싸이고, 고정된 길이로 제한된다.
고정된 길이[``77``]보다 짧은 시퀀스는 padding 토큰으로 채워지고, 초과하면 시퀀스가 잘려나간다.

#### ➡️ Training
image-text pair에 대해 CLIP은 matched pair면 <font style="color:hsl(27, 100%, 43%)">cosine similarity</font>를 최대화시키고, unmatched pair면 최소화 시킨다.

#### ➡️ Zero-shot Inference
CLIP은 웹 기반으로 다양하고 많은 text prompt로부터 학습이 되어 있다. 그렇기에 zero-shot inference 성능이 높다.

<center>
<img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/8dca4094-824b-49bc-b6da-fbc0ec8c0c68" width="50%" height="50%"></center>

```
f : image encoder를 통과하여 얻은 feature
K : downstream task의 클래스 개수
cos : cosine similarity
w_i : text encoder를 통과하여 얻은 weight
```

### [2] Context Optimization (CoOp)
pre-trained model의 parameter를 고정한 상태로, 데이터로부터 <font style="color:hsl(27, 100%, 43%)">continuous vector로 context word</font>를 모델링하여 manual prompt tuning(수동 프롬프트 조정)을 피하는 방법이다.

구현 방법은 두 가지이다.

#### ➡️ Unified Context
모든 클래스에 same context를 공유하는 방법이다.
- ``[CLASS]`` 토큰을 수식 <font style="color:hsl(27, 100%, 43%)">끝 [end]</font>에  넣을 수도 있고,    ex) ``a photo of a [CLASS]``
- 수식 <font style="color:hsl(27, 100%, 43%)">중간 [mid]</font>에 넣을 수도 있다.     ex) ``a photo of a [CLASS], a type of object``

<center>
<img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/15213b16-f1ef-4206-be51-3f6789a02643" width="50%" height="50%"/></center>

#### ➡️ Class-Specific Context (CSC)
각 클래스에 대해 독립적인 context vectors를 사용한다. CSC는 <font style="color:hsl(27, 100%, 43%)">fine-grained classification task</font>에 유리하다.

<br>

## 🦾 Experiments
사용한 데이터셋은 총 11개이다. ➡️ ImageNet, Caltech101, Oxford-Pets, StanfordCars, Flowers102, Food101, FGVCAircraft, SUN397, DTD, EuroSAT, UCF101

실험은 <font style="color:hsl(27, 100%, 43%)">class token의 position</font>과 <font style="color:hsl(27, 100%, 43%)">구현 방식</font>, <font style="color:hsl(27, 100%, 43%)">context length</font>에 따라 각각 진행된다.

backbone에 대한 실험을 제외하고는 모두 ResNet-50을 사용하였다. CoOp's context vectors는 무작위로 초기화하였으며, 그 외의 train 설정은 논문을 참고하길 바란다.

### 📈 Hand-Crafted prompts와 비교
<center>
<img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/40138618-8b6c-4396-bafd-37c2451f0820" width="50%" height="50%"/></center>

CoOp's context length를 16으로 지정했을 때, Food101 Dataset을 제외하고는 모두 zero-shot CLIP보다 성능이 1.24~45.97% 향상된 것을 확인할 수 있다.

### 📈 Linear Prob CLIP과의 비교
<center>
<img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/9717a9f1-f4c9-4ed1-814e-d98e989ed237" width="40%" height="40%"/></center>

Linear Probing이 CLIP+CoOp(end/mid, CSC) 보다 높은 성능을 보인 Dataset들이 있었지만, 평균적으로 보았을 때 Linear Probing보다 CoOp 방식을 사용하였을 때, 성능이 더 우수한 것을 볼 수 있다.

<center>
<img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/b7a2c304-8e6a-4b0a-b015-1f84e516c102" width="50%" height="50%"/></center>

위 table은 <font style="color:hsl(27, 100%, 43%)">Domain Shift</font>에 대한 성능 확인으로 Linear Probing보다 대략 7~10% 정도 높은 robustness를 보인다.

### 📈 Unified vs Class-Specific Context

<center>
<img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/3323e66b-4680-4e7f-868c-d03125d4fd26" width="30%" height="30%"/></center>

논문의 fig3을 참고한 결과, 대게 <font style="color:hsl(27, 100%, 43%)">Unified Context</font> 방식이 더 좋은 성능을 내었다. 그러나 더 높은 sample 수에서 ``Flower102, FGVAircraft, DTD, EuroSAT``에서의 결과는 **CSC**가 더 높은 성능을 보이는 것을 확인할 수 있었다.

### 📈 Context Length
[Domain Shift에서의 성능을 언급했던 Figure](https://github.com/m2nja201/m2nja201.github.io/assets/80443295/b7a2c304-8e6a-4b0a-b015-1f84e516c102)에서는 ``M=4``(<font style="color:hsl(27, 100%, 43%)">shorter context length</font>)에서의 성능이 ``M=16``일 때의 성능보다 높은 것을 볼 수 있었다. ➡️ 이는 적은 파라미터를 사용하여 <font style="color:hsl(27, 100%, 43%)">더 적은 오버피팅</font>을 보였기 때문이다.

그러나 일반 Source에 대한 성능을 확인해본 결과 <font style="color:hsl(27, 100%, 43%)">context token이 더 많을 수록</font> 성능이 좋은 것을 확인할 수 있었다.

<center>
<img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/547625cf-9ba2-4bee-87fa-00a07c5c4a27" width="40%" height="40%"/> <img src="https://github.com/m2nja201/m2nja201.github.io/assets/80443295/9b623590-10d5-4817-a1e7-a5160fa8bd96" width="40%" height="40%"/></center>

> 논문에서는 'context 길이가 길수록 token을 중간(mid)에 위치시키는 것이 더 좋다'라고 주장을 하지만, 그래프에서 확인했을 때는 말과 일치하지가 않아 이 부분은 혼동이 옵니다. 아시는 분들은 피드백 부탁합니다!

결론적으로는, domain shift와 성능 사이에서의 context length 황금 비율은 없다는 것이다.

### 📈 그 외의 부수적인 Results
- Vision Backbone : backbone 구조가 성장할 수록 더 좋은 성능을 내었으며, backbone 구조와 무관하게 CoOp 방식이 성능 향상을 잘 이끌었다는 것을 보였다.
- Prompt Ensembling과의 비교 : prompt engineering < prompt ensembling < CoOp으로 성능이 좋았으며, prompt ensembling보다 2.54~4.07% 높은 성능을 보였다.
- 다른 Fine Tuning과의 비교 : text encoder를 통과하는 gradient가 더 유용한 정보를 제공하는 것을 확인할 수 있었고, CoOp의 성능이 가장 우수함을 확인할 수 있었다.

<br>

## 🦾 Conclusions
- 대규모 Vision model을 prompt learning으로 적응시키는 첫 번째 종합적인 연구이다.
- Domain 일반화와 기본적인 성능이 수동 prompt보다 훨씬 높은 성능을 도출한다.
- Continous 방식을 채택하였기 때문에, 해석하기가 상대적으로 어렵다.
- Food101 dataset에서 약한 성능을 보인 것을 보아, noise가 많은 label에 민감하다.

<br>


## 🦾 CoOp 논문에 대한 나의 생각
prompt 자체를 **자동화**한 것이 매우 혁신적인 아이디어라고 생각되었다. 이를 통해 **domain 일반화**에서 기존 연구된 기법들보다 훨씬 높은 성능을 얻은 것이 가장 인상 깊었던 것 같다. 아쉬운 점이 있다면, 논문의 명확성이 부족했던 것 같다.
그러나 Abstract부터 Introduction, Method 곳곳에 핵심적이라고 생각되는 부분들을 여러 번 언급하여 저자가 말하고자 하는 핵심 내용들과 CoOp의 기여를 알 수 있었다.

## 🦾 CoOp 기법이 코드에 어떻게 녹여져 있는지 단순 분석해보기
[[Paper's Code Review] CoOp 코드 간단 분석](https://m2nja201.github.io/computervision/paperreview/multimodal/CoOp-code/)

<font style="color:rgb(60, 60, 60)">※ 위 링크를 누르면 게시글로 이동합니다.</font>
