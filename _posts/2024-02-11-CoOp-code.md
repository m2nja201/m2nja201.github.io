---
title:  "[Paper's Code Review] CoOp 코드 간단 분석"
excerpt: "Learning to Prompt for Vision-Language Models (CoOp) 코드 간단 분석"

toc: true
toc_sticky: true

categories:
  - ComputerVision 
  - PaperReview
  - MultiModal

---

논문 <font style="color:hsl(27, 100%, 43%)">Learning to Prompt for Vision-Language Models</font>을 읽고, 
CoOp 기법들이 코드에서 어떻게 구현이 되었는지, 간단히 분석할 예정입니다.

보시고 피드백도 자유롭게 주셨으면 좋겠습니다!

## 📜 논문 분석 먼저 보기
[[Paper Review] CoOp :: Learning to Prompt for Vision-Language Models](https://m2nja201.github.io/computervision/paperreview/multimodal/CoOp/)

<font style="color:rgb(60, 60, 60)">※ 위 링크를 누르면 게시글로 이동합니다.</font>

<br>

## 🦾 CoOp 기법이 반영된 코드 단순 분석
KaiyangZhou의 CoOp open code : [https://github.com/KaiyangZhou/CoOp/](https://github.com/KaiyangZhou/CoOp/)

📁``CoOp/trainers/coop.py`` > ⚙️``class PromptLearner`` : CLIP 모델의 prompt 부분을 <font style="color:hsl(27, 100%, 43%)">학습 가능</font>하게 만들고, 각 클래스에 맞는 <font style="color:hsl(27, 100%, 43%)">prompt를 생성</font>

<script src="https://gist.github.com/m2nja201/039d134de78763232c0110c246fd271b.js"></script>

📁``CoOp/trainers/coop.py`` > ⚙️``class CustomCLIP`` : 이미지 인코더와 텍스트 인코더, <font style="color:hsl(27, 100%, 43%)">prompt_learner</font>를 포함한다. 이미지와 프롬프트를 통해 생성된 텍스트 feature 사이의 매칭을 수행한다.

<script src="https://gist.github.com/m2nja201/937421ea0adcde630c821b0c7fcd081c.js"></script>

📁``CoOp/trainers/coop.py`` > ⚙️``class CoOp`` : <font style="color:hsl(27, 100%, 43%)">CoOp의 방법론</font>을 구현한 class이다.

<script src="https://gist.github.com/m2nja201/03d94b89101efad992549be56e5956be.js"></script>